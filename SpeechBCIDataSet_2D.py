"""
This module defines the SpeechBCIDataSet_2D class, a custom PyTorch Dataset
designed for handling Speech BCI Array Recordings. Each 2D array is treated
as an independent sample, and the dataset is generated by flattening these
2D arrays into 1D arrays.

Classes:
    SpeechBCIDataSet_2D: A custom PyTorch Dataset for Speech BCI Array Recordings.

Usage example:
    dataset = SpeechBCIDataSet_2D(etl_dir="/path/to/etl")
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
    for data in dataloader:
        # process data
"""

import numpy as np
import os
import pandas as pd

import torch
from torch.utils.data import Dataset

from SpeechBCI import ElectrodeArray
from Sentence import Sentence

class SpeechBCIDataSet_2D(Dataset):
    """
    PyTorch custom Dataset tuned for Speech BCI Array Recordings.
    
    We treat each 2D array as an independent sample.
    
    And in this simple starting point, we'll flatten the 2D to 1D.
    
    That means that one input file will generate multiple samples.
    To do this, we spoof the classic sample, label pair in gen_dataset.
    
    Note:
        Adhere to the convention of NTCHW.

    Args:
        etl_dir (str): Directory containing the ETL files.
        transform (callable, optional): Optional transform to be applied on a sample.
        target_transform (callable, optional): Optional transform to be applied on the target.
    """
    
    def __init__(self, etl_dir, transform=None, target_transform=None):
        self.samples = self.gen_dataset(etl_dir)
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.samples)

    def gen_dataset(self, etl_dir):
        """
        Generate the dataset by reading ETL files from the specified directory.

        Args:
            etl_dir (str): Directory containing the ETL files.

        Returns:
            np.ndarray: Array of samples.
        """
        basefile_names = [f.split('.')[0] for f in os.listdir(etl_dir) if f.endswith('.csv')]
        idkeys = []
        var_names = []
        for basefile in basefile_names:
            parts = basefile.split('_')
            if not (parts[-1] == 'sentenceText'):
                idkeys.append('_'.join(parts[0:-3]))
                var_names.append('_'.join(parts[-3:]))
        idkeys = list(set(idkeys))
        var_names = set(var_names)
        print(f"Found {len(idkeys)} unique idkeys and {len(var_names)} unique variable names {var_names}.")
        var_names = list(var_names)
        
        samples = []
        for idkey in idkeys:
            working_array = []
            for var_name in var_names:
                fname = os.path.join(etl_dir + os.sep + idkey + '_' + var_name + '.csv')
                x = ElectrodeArray()
                x.load(fname)
                working_array.append(x.xt.reshape(-1, x.num_rows, x.num_cols))
                
            working_array = np.stack(working_array, axis=1)
            samples.append(working_array)
            
        samples = np.concatenate(samples, axis=0)
 
        return samples
    
    def __getitem__(self, idx):
        """
        Get a sample from the dataset.

        Args:
            idx (int): Index of the sample to retrieve.

        Returns:
            torch.Tensor: The sample at the specified index.
        """
        x = torch.tensor(self.samples[idx], dtype=torch.float32)
        return x